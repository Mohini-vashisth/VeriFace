{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data Loading*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'README.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     10\u001b[0m metadata_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREADME.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[43mload_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(metadata)  \u001b[38;5;66;03m# Inspect metadata structure\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m, in \u001b[0;36mload_metadata\u001b[1;34m(metadata_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_metadata\u001b[39m(metadata_path):\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metadata\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'README.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load metadata from JSON file\n",
    "def load_metadata(metadata_path):\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    return metadata\n",
    "\n",
    "# Example usage\n",
    "metadata_path = \"README.json\"\n",
    "metadata = load_metadata(metadata_path)\n",
    "print(metadata)  # Inspect metadata structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def load_video(video_path, max_frames=100, target_size=(224, 224)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    while cap.isOpened() and len(frames) < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = cv2.resize(frame, target_size)\n",
    "        frame = frame / 255.0\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def process_videos_with_metadata(video_dir, metadata, max_frames=100, target_size=(224, 224)):\n",
    "    videos = []\n",
    "    labels = []\n",
    "    additional_features = []\n",
    "\n",
    "    for video_file in os.listdir(video_dir):\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        \n",
    "        # Extract metadata for the video\n",
    "        video_id = os.path.splitext(video_file)[0]\n",
    "        video_metadata = metadata.get(video_id, {})\n",
    "        \n",
    "        frames = load_video(video_path, max_frames, target_size)\n",
    "        videos.append(frames)\n",
    "        labels.append(video_metadata.get('label', 0))  # Default to 0 if no label\n",
    "        additional_features.append(video_metadata.get('features', {}))  # Adjust based on your metadata structure\n",
    "    \n",
    "    return np.array(videos), np.array(labels), additional_features\n",
    "\n",
    "# Example usage\n",
    "video_dir = '/Users/lakshya/Desktop/Projects/VeriFace/Video Dataset/train_data'\n",
    "metadata = load_metadata(metadata_path)\n",
    "videos, labels, additional_features = process_videos_with_metadata(video_dir, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data Extraction*\n",
    "1. Video Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_dir, frame_rate=1):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % frame_rate == 0:\n",
    "            cv2.imwrite(os.path.join(output_dir, f\"frame_{count}.jpg\"), frame)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "\n",
    "def process_videos(video_dir, output_base_dir, frame_rate=1):\n",
    "    if not os.path.exists(output_base_dir):\n",
    "        os.makedirs(output_base_dir)\n",
    "    \n",
    "    # Get all video files in the specified directory\n",
    "    video_files = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "    \n",
    "    # Limit to 400 videos\n",
    "    video_files = video_files[:400]\n",
    "    \n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        output_dir = os.path.join(output_base_dir, os.path.splitext(video_file)[0])\n",
    "        extract_frames(video_path, output_dir, frame_rate)\n",
    "        print(f\"Processed {video_file}\")\n",
    "\n",
    "\n",
    "process_videos('/Users/lakshya/Desktop/Projects/VeriFace/Video Dataset/train_data', '/Users/lakshya/Desktop/Projects/VeriFace/Extracted Video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Audio Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def extract_audio(video_path, output_audio_path):\n",
    "    audio = AudioSegment.from_file(video_path, format=\"mp4\")\n",
    "    audio.export(output_audio_path, format=\"wav\")\n",
    "\n",
    "def process_audios(video_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Get all video files in the specified directory\n",
    "    video_files = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "    \n",
    "    # Limit to 400 videos\n",
    "    video_files = video_files[:400]\n",
    "    \n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        output_audio_path = os.path.join(output_dir, os.path.splitext(video_file)[0] + '.wav')\n",
    "        extract_audio(video_path, output_audio_path)\n",
    "        print(f\"Extracted audio from {video_file}\")\n",
    "\n",
    "\n",
    "process_audios('/Users/lakshya/Desktop/Projects/VeriFace/Video Dataset/train_data', '/Users/lakshya/Desktop/Projects/VeriFace/Extracted Audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Preprocessing*\n",
    "1. Video Preprocessing: Resizing, Normalizing, Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def preprocess_video_frame(frame, target_size=(224, 224)):\n",
    "    frame = tf.image.resize(frame, target_size)\n",
    "    frame = tf.cast(frame, tf.float32) / 255.0  # Normalize\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Audio Preprocessing: Resampling, Noise reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def preprocess_audio(audio_path, target_sr=16000):\n",
    "    y, sr = librosa.load(audio_path, sr=target_sr)\n",
    "    y = librosa.effects.trim(y)[0]  # Trim silence\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Model Creation*\n",
    "1. Video Models: ResNet, VGG-16, C3D, TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50, VGG16\n",
    "\n",
    "def create_resnet_model(input_shape=(224, 224, 3)):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')  # Modify for your task\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "video_model = create_resnet_model()\n",
    "video_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_vgg16_model(input_shape=(224, 224, 3), num_classes=10):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  \n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')  \n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "vgg16_model = create_vgg16_model()\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_c3d_model(input_shape=(16, 112, 112, 3), num_classes=10):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv3D(64, kernel_size=(3, 3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "\n",
    "    model.add(layers.Conv3D(128, kernel_size=(3, 3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "    model.add(layers.Conv3D(256, kernel_size=(3, 3, 3), activation='relu'))\n",
    "    model.add(layers.Conv3D(256, kernel_size=(3, 3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "    model.add(layers.Conv3D(512, kernel_size=(3, 3, 3), activation='relu'))\n",
    "    model.add(layers.Conv3D(512, kernel_size=(3, 3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "    model.add(layers.Conv3D(512, kernel_size=(3, 3, 3), activation='relu'))\n",
    "    model.add(layers.Conv3D(512, kernel_size=(3, 3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "c3d_model = create_c3d_model()\n",
    "c3d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_tcn_model(input_shape=(128, 3), num_classes=10):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # TCN block 1\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=3, padding='causal', dilation_rate=1, activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=3, padding='causal', dilation_rate=2, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # TCN block 2\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=3, padding='causal', dilation_rate=4, activation='relu'))\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=3, padding='causal', dilation_rate=8, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "tcn_model = create_tcn_model(input_shape=(128, 3))  # Modify the input shape based on your data\n",
    "tcn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Audio Models: Wav2Vec, CRNN, VGGish, WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "# Load pre-trained Wav2Vec2.0 model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Load and preprocess audio\n",
    "def extract_wav2vec2_features(audio_path):\n",
    "    # Load audio file with librosa\n",
    "    y, sr = librosa.load(audio_path, sr=16000)  # Wav2Vec2 requires 16kHz sample rate\n",
    "    \n",
    "    # Preprocess the audio to match the input format for Wav2Vec2.0\n",
    "    input_values = processor(y, return_tensors=\"pt\", sampling_rate=sr).input_values\n",
    "    \n",
    "    # Extract features (output from the last hidden layer)\n",
    "    with torch.no_grad():\n",
    "        features = model(input_values).last_hidden_state\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Example function to extract Mel-spectrogram features from audio\n",
    "def extract_mel_spectrogram(audio_path, n_mels=128):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "# CRNN model\n",
    "def create_crnn_model(input_shape=(128, 128, 1), num_classes=10):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # CNN layers\n",
    "    model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # RNN layers (GRU or LSTM)\n",
    "    model.add(layers.Reshape(target_shape=(-1, 128)))\n",
    "    model.add(layers.GRU(128, return_sequences=False))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "crnn_model = create_crnn_model(input_shape=(128, 128, 1))  # input shape should match the Mel-spectrogram shape\n",
    "crnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def residual_block(x, dilation_rate):\n",
    "    conv = layers.Conv1D(filters=64, kernel_size=2, padding='causal', dilation_rate=dilation_rate)(x)\n",
    "    conv = layers.Activation('relu')(conv)\n",
    "    conv = layers.Conv1D(filters=64, kernel_size=2, padding='causal')(conv)\n",
    "    \n",
    "    # Residual connection\n",
    "    x = layers.add([x, conv])\n",
    "    return x\n",
    "\n",
    "def create_wavenet_model(input_shape=(16000, 1), num_classes=10):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv1D(filters=64, kernel_size=2, padding='causal')(inputs)\n",
    "    \n",
    "    # Stack of residual blocks with increasing dilation rates\n",
    "    for dilation_rate in [1, 2, 4, 8, 16]:\n",
    "        x = residual_block(x, dilation_rate)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "wavenet_model = create_wavenet_model()\n",
    "wavenet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def extract_audio(video_path, output_audio_path):\n",
    "    audio = AudioSegment.from_file(video_path, format=\"mp4\")\n",
    "    audio.export(output_audio_path, format=\"wav\")\n",
    "\n",
    "def process_audios(video_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Get all video files in the specified directory\n",
    "    video_files = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "    \n",
    "    # Limit to 400 videos\n",
    "    video_files = video_files[:400]\n",
    "    \n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        output_audio_path = os.path.join(output_dir, os.path.splitext(video_file)[0] + '.wav')\n",
    "        extract_audio(video_path, output_audio_path)\n",
    "        print(f\"Extracted audio from {video_file}\")\n",
    "\n",
    "# Example usage\n",
    "process_audios('input_videos', 'audio_files')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Feature Extraction*\n",
    "1. Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vgg16_features(video_frames):\n",
    "    vgg16_model = create_vgg16_model()\n",
    "    vgg16_features = vgg16_model.predict(video_frames)\n",
    "    return vgg16_features\n",
    "\n",
    "def extract_resnet50_features(video_frames):\n",
    "    resnet_model = create_resnet_model()\n",
    "    resnet_features = resnet_model.predict(video_frames)\n",
    "    return resnet_features\n",
    "\n",
    "def extract_c3d_features(video_frames):\n",
    "    c3d_model = create_c3d_model()\n",
    "    c3d_features = c3d_model.predict(video_frames)\n",
    "    return c3d_features\n",
    "\n",
    "def extract_tcn_features(mel_spectrogram):\n",
    "    tcn_model = create_tcn_model()\n",
    "    tcn_features = tcn_model.predict(mel_spectrogram)\n",
    "    return tcn_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_crnn_features(mel_spectrogram):\n",
    "    crnn_model = create_crnn_model()\n",
    "    crnn_features = crnn_model.predict(mel_spectrogram)\n",
    "    return crnn_features\n",
    "\n",
    "def extract_wavenet_features(audio_data):\n",
    "    wavenet_model = create_wavenet_model()\n",
    "    wavenet_features = wavenet_model.predict(audio_data)\n",
    "    return wavenet_features\n",
    "\n",
    "def extract_vggish_features(audio_data):\n",
    "    vggish_model = hub.load(\"https://tfhub.dev/google/vggish/1\")\n",
    "    vggish_features = vggish_model(audio_data)\n",
    "    return vggish_features\n",
    "\n",
    "def extract_wav2vec_features(audio_data):\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    input_values = processor(audio_data, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "    with torch.no_grad():\n",
    "        wav2vec_features = model(input_values).last_hidden_state\n",
    "    return wav2vec_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Feature Fusion*\n",
    "1. Video \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "def fuse_video_features(vgg16_features, resnet50_features, c3d_features, tcn_features):\n",
    "    fused_features = concatenate([vgg16_features, resnet50_features, c3d_features, tcn_features], axis=-1)\n",
    "    return fused_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "def fuse_audio_features(crnn_features, wavenet_features, vggish_features, wav2vec_features):\n",
    "    fused_features = concatenate([crnn_features, wavenet_features, vggish_features, wav2vec_features], axis=-1)\n",
    "    return fused_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Final Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_model(video_shape, audio_shape, num_classes=10):\n",
    "    # Create video and audio feature extraction models\n",
    "    vgg16_model = create_vgg16_model()\n",
    "    resnet50_model = create_resnet_model()\n",
    "    c3d_model = create_c3d_model()\n",
    "    \n",
    "\n",
    "    crnn_model = create_crnn_model()\n",
    "    wavenet_model = create_wavenet_model()\n",
    "    vggish_model = hub.load(\"https://tfhub.dev/google/vggish/1\")\n",
    "    \n",
    "    # Define input layers\n",
    "    video_input = layers.Input(shape=video_shape)\n",
    "    audio_input = layers.Input(shape=audio_shape)\n",
    "    \n",
    "    # Extract features\n",
    "    vgg16_features = vgg16_model(video_input)\n",
    "    resnet50_features = resnet50_model(video_input)\n",
    "    c3d_features = c3d_model(video_input)\n",
    "    \n",
    "    crnn_features = crnn_model(audio_input)\n",
    "    wavenet_features = wavenet_model(audio_input)\n",
    "    vggish_features = vggish_model(audio_input)\n",
    "    \n",
    "    # Fuse video features\n",
    "    fused_video_features = fuse_video_features(vgg16_features, resnet50_features, c3d_features)\n",
    "    \n",
    "    # Fuse audio features\n",
    "    fused_audio_features = fuse_audio_features(crnn_features, wavenet_features, vggish_features)\n",
    "    \n",
    "    # Combine fused features\n",
    "    combined_features = concatenate([fused_video_features, fused_audio_features], axis=-1)\n",
    "    \n",
    "    # Final classification model\n",
    "    x = layers.Dense(256, activation='relu')(combined_features)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=[video_input, audio_input], outputs=outputs)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
